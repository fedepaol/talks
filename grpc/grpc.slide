RPC on steroids with Go and Grpc

Federico Paolinelli
Red Hat
@fedepaol
fedepaol@gmail.com
fpaoline@redhat.com

* About me

- Red Hatter
- Doing distributed systems for more than 15 years
- CNV Networking team -> KubeVirt
- Open source lover

* Let's talk about RPC

* All but a new thing

- Corba
- Wsdl
- Java RMI
- Com (?)

:  allow function callers and function implementers to live in different processes
: the client and the server in an RPC exchange must be up and running at the same time.
: language indpendent definition of the Rpc -> IDL Interface domain language

* The need for communication

.image images/mailboxes.jpg 400 _
: TODO Creative commons
: not only Frontend / backend
: due to the complexity of our systems we need the appropriate tools. Go beyond simple remote invokation

* What options do I have?

- rest
- other rpcs (thrift, avro)
- websockets
- framed tcp
: rest is the king here. Not type safe. I know there are tools to generate the client /server but still
: websockets is not really a standard
: framed tcp: NIH syndrome

* Enters Grpc
: stands for Grpc remote procedure calls
: iteration of google's stubby, then opensourced

* Grpc & http2

Http2 allows multiplexing of requests over a single tcp connection (streams)

Grpc introduces channels, rpcs and messages:

.image images/grpc_on_http2_mapping_2.png 400 _

: Many streams on a single connection. On each stream, many messages (binary format)
: Advantage of a stream -> connection concurrency. Interleave messages on a single connection
: in go channel renamed to client conn
: channels enable multiple streams over multiple http2 connections (if load balance api is used)

* Serialization

* Protocol Buffers

*Types*definition*

    message Beer {
        int32 bid = 1;
        string beer_name = 2;
        string beer_description = 3;
        string beer_style = 4;
    }

*Services*definition*

    service BeersService {
        rpc GetBeer(BeerID) returns (Beer);
        rpc QueryBeer(BeerQueryParams) returns (stream Beer);
    }


: Default serialization mechanism for Grpc
: Run the protoc compiler and get data classes + serialization / deserialization
: sync call, streamed result, streamed in
: code generation for a lot of languages, interoperability

* Protobufs favour backward compatibility

- New fields can be added without breaking retro compatibility
- Intermediate service can parse the data, check what they need and propagate the data again

: be careful in setting the default values 

* Server side

    type BeersServiceServer interface {
        GetBeer(context.Context, *BeerID) (*Beer, error)
        QueryBeer(*BeerQueryParams, BeersService_QueryBeerServer) error
    }

*Client*side*
    
    // Generated code
    func NewBeersServiceClient(cc *grpc.ClientConn) BeersServiceClient
    func (c *beersServiceClient) GetBeer(ctx context.Context, in *BeerID, opts ...grpc.CallOption) (*Beer, error)

* Server implementation

    func (b *BeerServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        beerID := id.GetBid()
        beer, ok := b.beers[int(beerID)]
        if !ok {
            // from google.golang.org/grpc/status
            return nil, status.Error(codes.NotFound, "beer not found")
        }
        return &beer, nil
    }

: convenience method to avoid npes
: grpc comes with a grpc/status package to provide rich errors

* Server implementation (streaming)

    func (b *BeerServer) QueryBeer(p *beer.BeerQueryParams, 
                                   s beer.BeersService_QueryBeerServer) error {
        for _, b := range b.beers {
            if strings.Contains(b.BeerDescription, p.GetQuery()) {
                s.Send(&b)
            }
        }
        return nil
    }

* Establishing a connection

Client side:

    conn, _ := grpc.Dial(serverAddr, grpc.WithInsecure())
    client := beer.NewBeersServiceClient(conn)
       
Server side:

    server := grpc.NewServer()
    beer.RegisterBeersServiceServer(server, &BeerServer{})
	
: Note that a connection can be in idle state, meaning that no rpc is invoked (or idle timeout has passed)
: a client connection can be used by many goroutines, all the gprc happen on separate goroutines
: there is also tls support 

* Keep alive

Not enabled by default, must use [[google.golang.org/grpc/keepalive]]

- Useful to detect disconnections where the endpoint hangs or dies
- Relies on http2 ping frames

Nice side effects:

- Keeps a pool of connections healthy
- Avoids killing idle of connections

: Tbh a really forgiving server side keep alive is enabled by default
: Needs to be enabled
: Some proxies kill connections where no traffic is passing

* Grpc Context gets propagated

* Deadline propagation

    ctx, cancel := context.WithTimeout(context.Background(), time.Second)
    beer, err := client.GetBeer(ctx, id)

.image images/propagation.png 200 _
    
    func (b *IntermediateServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        if ctx.Err() == context.Canceled {
            return status.New(codes.Canceled, "Client cancelled, abandoning.")
        }
        b.client.GetBeer(ctx, id)
    }


: deadline sets a point in time, the same for all the calls
: with timeout sets a deadline in the future.
: if the same context gets propagated the calls will expire without wasting resources.
: the server can check if the client is still interested before wasting resources. Can decide to proceed, if the request is cached 
: and idempotency is implemented

* Cancellation

    ctx, cancel := context.WithCancel(context.Background())
    done := make(chan bool, 0)
    go func() {
        client.GetBeer(ctx, id)
        close(done)
    }()
    cancel()

.image images/cancellation.png 200 _


: cancellation gets propagated through te calls

* Metadata

Regular context values are not propagated.

[[google.golang.org/grpc/metadata]] is.

Client:

    metaData := metadata.Pairs("key", "value")
    ctx := metadata.NewContext(context.Background(), metaData)

Server:

    meta, ok := metadata.FromContext(ctx)
    value : = meta["key"][0]

* Interceptors

* Authentication

* Tracing

* Monitoring


* Json interoperability (grpc gateway?)