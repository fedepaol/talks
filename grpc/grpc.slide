RPC on steroids with Go and Grpc

Federico Paolinelli
Red Hat
@fedepaol
fedepaol@gmail.com
fpaoline@redhat.com

* About me

- Red Hatter
- Doing distributed systems for more than 15 years
- KubeVirt networking team
- Open source lover

* Examples

Examples of all the concepts described during this talk can be found at
[[https://github.com/fedepaol/talks/tree/master/grpc/examples]]

* Let's talk about RPC

* Prior art

- Corba
- Wsdl
- Java RMI
- Com (?)

:  allow function callers and function implementers to live in different processes
: the client and the server in an RPC exchange must be up and running at the same time.
: language indpendent definition of the Rpc -> IDL Interface domain language

* The need for communication

.image images/mailboxes.jpg 400 _
: TODO Creative commons
: demand for interoperability. In microservices, there is a push towards using different technology stacks
: Mobile also introduced need for efficiency
: not only Frontend / backend
: due to the complexity of our systems we need the appropriate tools. Go beyond simple remote invokation

* What options do I have?

- rest
- other rpcs (thrift, avro)
- websockets
- framed tcp
: rest is the king here. Not type safe. I know there are tools to generate the client /server but still
: websockets is not really a standard
: framed tcp: NIH syndrome

* Enters Grpc
: stands for Grpc remote procedure calls
: iteration of google's stubby, then opensourced

* Grpc & http2

Http2 allows multiplexing of requests over a single tcp connection (streams)

Grpc introduces channels, rpcs and messages:

.image images/grpc_on_http2_mapping_2.png 500 _

: Many streams on a single connection. On each stream, many messages (binary format)
: Advantage of a stream -> connection concurrency. Interleave messages on a single connection
: in go channel renamed to client conn
: channels enable multiple streams over multiple http2 connections (if load balance api is used)

* Serialization

* Protocol Buffers

*Types*definition*

    message Beer {
        int32 bid = 1;
        string beer_name = 2;
        string beer_description = 3;
        string beer_style = 4;
    }

*Services*definition*

    service BeersService {
        rpc GetBeer(BeerID) returns (Beer);
        rpc QueryBeer(BeerQueryParams) returns (stream Beer);
    }


: Default serialization mechanism for Grpc
: Binary serialization protocol
: Run the protoc compiler and get data classes + serialization / deserialization
: sync call, streamed result, streamed in
: use for streaming: pagination (memory efficient) OR pub / sub
: code generation for a lot of languages, interoperability
: two parts: types & endpoints
: there is a way to marshal from / to json. Useful for debugging. not efficient

* Protobufs favour backward compatibility

- New fields can be added without breaking retro compatibility
- Intermediate service can parse the data, check what they need and propagate the data again

: Protobuf has a list of safe changes,what happens if a type gets converted in another type.
: be careful in setting the default values 

* protoc beer.proto --go_out=plugins=grpc:pkg/beer

* Server side

    type BeersServiceServer interface {
        GetBeer(context.Context, *BeerID) (*Beer, error)
        QueryBeer(*BeerQueryParams, BeersService_QueryBeerServer) error
    }

*Client*side*
    
    // Generated code
    func NewBeersServiceClient(cc *grpc.ClientConn) BeersServiceClient
    func (c *beersServiceClient) GetBeer(ctx context.Context, 
                                         in *BeerID, 
                                         opts ...grpc.CallOption) (*Beer, error)

* Establishing a connection

Client side:

    conn, _ := grpc.Dial(serverAddr, grpc.WithInsecure())
    client := beer.NewBeersServiceClient(conn)
       
Server side:

    lis, _ := net.Listen("tcp", port)
    server := grpc.NewServer()
    beer.RegisterBeersServiceServer(server, &BeerServer{})
    server.Serve(lis)

: Note that a connection can be in idle state, meaning that no rpc is invoked (or idle timeout has passed)
: a client connection can be used by many goroutines, all the gprc happen on separate goroutines
: there is also tls support 

* Keep alive

Not enabled by default, must use [[https://google.golang.org/grpc/keepalive]]

- Useful to detect disconnections where the endpoint hangs or dies
- Relies on http2 ping frames

Nice side effects:

- Keeps a pool of connections healthy
- Avoids killing idle of connections

: grpc hides the connection complexity 
: Normally, when there is no activity the connection state switches from ready to idle
: Tbh a really forgiving server side keep alive is enabled by default
: Needs to be enabled
: Some proxies kill connections where no traffic is passing
: Needed only in case of idle connections
: There is also connection max age on server side

* Server implementation

    func (b *BeerServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        beerID := id.GetBid()
        beer, ok := b.beers[int(beerID)]
        if !ok {
            // from google.golang.org/grpc/status
            return nil, status.Error(codes.NotFound, "beer not found")
        }
        return &beer, nil
    }

: convenience method to avoid npes
: grpc comes with a grpc/status package to provide rich errors

* Server implementation (streaming)

    func (b *BeerServer) QueryBeer(p *beer.BeerQueryParams, 
                                   s beer.BeersService_QueryBeerServer) error {
        for _, b := range b.beers {
            if strings.Contains(b.BeerDescription, p.GetQuery()) {
                s.Send(&b)
            }
        }
        return nil
    }

* Error Handling

* Error Handling

    func (b *BeerServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        /* ... */
        return nil, fmt.Error("Failed to get a beer")
    }

On the client side:

    beer, err := client.GetBeer(ctx, id)
    if err != nil && err.Error() == "Failed to get a beer" {
        // handle missing beer
    }

* Error Handling

    func (b *BeerServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        /* ... */
        return nil, status.Error(codes.NotFound, "beer not found")
    }

On the client side:

    beer, err := client.GetBeer(ctx, id)
    if err != nil {
        st := status.Convert(err)
        if st != nil && st.Code() == codes.NotFound && st.Message() == "beer not found" {
            // handle missing beer
        }
    }
: grpc comes with a grpc/status package to provide rich errors

* Error Handling

    func (b *BeerServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        /* ... */
        beerErr := beer.BeerError{
            Reason:  beer.BeerErrorCode_BEER_NOT_FOUND,
            Message: fmt.Sprintf("Could not find beer %v", *id),
        }
        st, _ := status.New(codes.NotFound, "not found").WithDetails(&beerErr)
        return nil, st.Err()
    }

On the client side:

    res, err := client.GetBeer(ctx, id)
    if err != nil {
        st := status.Convert(err)
        for _, detail := range st.Details() {
            switch t := detail.(type) {
                case *beer.BeerError:
                    // handle t payload
            }
        }
    }

: Type safe error. We know that the protobuf is shared between the client and the server so it's a more convenient
: way to handle errors


* Grpc Context gets propagated

* Deadline propagation

    ctx, cancel := context.WithTimeout(context.Background(), time.Second)
    beer, err := client.GetBeer(ctx, id)

.image images/propagation.png 200 _
    
    func (b *MiddleServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        if ctx.Err() == context.Canceled {
            return status.New(codes.Canceled, "Client cancelled, abandoning.")
        }
        b.client.GetBeer(ctx, id)
    }


: deadline sets a point in time, the same for all the calls
: with timeout sets a deadline in the future.
: if the same context gets propagated the calls will expire without wasting resources.
: the server can check if the client is still interested before wasting resources. Can decide to proceed, if the request is cached 
: and idempotency is implemented

* Cancellation

    ctx, cancel := context.WithCancel(context.Background())
    done := make(chan bool, 0)
    go func() {
        client.GetBeer(ctx, id)
        close(done)
    }()
    cancel()

.image images/cancellation.png 200 _

: cancellation gets propagated through te calls

* Metadata

Regular context values are not propagated.

[[https://google.golang.org/grpc/metadata]] is.

Client:

    md := metadata.Pairs("key", "value")
	ctx := metadata.NewOutgoingContext(context.Background(), md)
    
Server:

	meta, ok := metadata.FromIncomingContext(ctx)
    value : = meta.Get("key")[0]

: If it's business logic the api should be changed.
: metadata should carry only "infrastructure" payload
: no static checks, no code generated. To handle with care.

* Interceptors

.image images/interceptors.jpg 500 _

* Interceptors

Same as http middleware.

Two types:

- Unary Interceptors
- Stream Interceptors

Both Client side / server side

: I won't add the signature here since it's quite long
: can apply to server side and client side. Http generally intercept server side
: client unary. Pre processing, call, post processing
: client streaming. Pre processing, retrieve the stream, optionally decorate it with interceptior methods, return decorated stream
: server side is almost the same but instead of invoker you have a handler to pass the data / the stream

* Interceptors

To apply:

Client:

    conn, err := grpc.Dial(*addr, grpc.WithUnaryInterceptor(unaryInterceptor))

Server:

    s := grpc.NewServer(grpc.UnaryInterceptor(unaryInterceptor))

If we want more interceptors (client side):

    func WithChainUnaryInterceptor(interceptors ...UnaryClientInterceptor) DialOption

Server side not natively supported. Must use [[github.com/grpc-ecosystem/go-grpc-middleware]].

: they are per conn / per server. this means that all the calls on that conn will attemtp a retry, or 
: authenticate. If we want special cases, we need to handle them in our code. One way to deal with it is
: to use context values 

* Retry unary interceptor

    func RetryInterceptor(ctx context.Context, 
                          method string, 
                          req, reply interface{}, 
                          cc *grpc.ClientConn, 
                          invoker grpc.UnaryInvoker, 
                          opts ...grpc.CallOption) error {
        var lastErr error
        for attempt := uint(0); attempt < 10; attempt++ {
            lastErr = invoker(ctx, method, req, reply, cc, opts...)
            if lastErr == nil {
                return nil
            }
        }
        return lastErr
    }

: three phases. The actual call happens in invoker
: more real scenario. Only some errors are worth being retried (the other end needs to be idempotent. Think about about
: a payement service that is not idempotent.) Also, not all the calls can be idempotent. The caller needs to tell the info
: to the interceptor. Logic on method name, arguments passed in the context (the real context) or as metadata.


* Rate limiter interceptor

    func UnaryServerInterceptor(ctx context.Context, 
                                req interface{}, 
                                info *grpc.UnaryServerInfo, 
                                handler grpc.UnaryHandler) (interface{}, error) {
        if Limit() {
            return nil, status.Errorf(codes.ResourceExhausted, 
                                     "%s is rejected by retry limiter.", info.FullMethod)
        }
        return handler(ctx, req)
    }

* Authentication (Client Side)

    func jwtInterceptor(token string) (UnaryClientInterceptor, err) {
        return func(ctx context.Context, 
                            method string,
                            req interface{},
                            reply interface{},
                            cc *grpc.ClientConn,
                            invoker grpc.UnaryInvoker,
                            opts ...grpc.CallOption) error {

            authCtx := metadata.AppendToOutgoingContext(ctx, "jwt", "bearer "+jwt.token)
            err := invoker(authCtx, method, req, reply, cc, opts...)
            if err != nil && status.Code(err) == codes.Unauthenticated {
                // handle unauthenticated
            }
            if err != nil {
                return err
            }
        }
    }

: we can leverage the interceptor to handle jwt tokens sending and validation
: this provides no way to refresh the token. Smarter ways:
: set the context metadata on per call basis
: have a struct bound to the interceptor
: here we use both the metadata and the interceptor on both sides.

* Authentication (Server Side)

    func authInterceptor(ctx context.Context, 
                        req interface{}, 
                        _ *grpc.UnaryServerInfo, 
                        handler grpc.UnaryHandler) (interface{}, error) {
        meta, ok := metadata.FromIncomingContext(ctx)
        if !ok {
            return nil, status.Error(codes.Unauthenticated, "INTERNAL_SERVER_ERROR")
        }
        if len(meta["jwt"]) != 1 {
            return nil, status.Error(codes.Unauthenticated, "INTERNAL_SERVER_ERROR")
        }
        token := meta["jwt"][0]
        // handle token validation
        return handler(ctx, req)
    }

* Interceptors pitfall

- once set, the interceptor are triggered for all the methods
- need to find a way to disable (some) interceptors on per method basis

: there are some methods that might not need authenticatoin
: there are some methods that are not idempotent (and for that reason you don't want to retry)

* Other interceptor goodies

- Tracing (with opentracing)
- Logging
- Monitoring

For more check [[https://github.com/grpc-ecosystem/go-grpc-middleware][grpc middleware]]

* Opentracing + Jaeger

.image images/opentracing.png 500 _

: no brainer. You just add the interceptors and all your calls get traced

* Service discovery & Load balancing

: Until now we saw we point the server using an address and a port. Very like what we do with 
: rest. Grpc offers more sophisticated ways to fetch the endpoints

* Name resolution

You have a name (URI) for the server, and want one (or more) addresses back.

Enters [[https://godoc.org/google.golang.org/grpc/resolver#Resolver][resolver.Resolver]]

Available off the box:

- dns
- passthrough
- manual

    passthrough:///localhost:50051

: normally it's dns, pick first. That's why you don't notice it
: the resolver is an object that gets instantiated, belongs to a given schema (i.e. dns), and
: returns a list of addresses (and address types)
: segway: The address type can be backend or grpclb

* Build your own resolver

We need to register a [[https://godoc.org/google.golang.org/grpc/resolver#Builder][resolver.Builder]]

- Build() method to return the resolver for the given target
- Scheme() method to declare the scheme we are registering against


Then update the  client connection has a method for updating 
    
    cc.UpdateState(resolver.State{Addresses: addrs})

Nice example: [[https://etcd.io/docs/v3.2.17/dev-guide/grpc_naming/][etcd name resolver]]

:   Two options for registering it:
:   	resolver.Register(&exampleResolverBuilder{}) register the resolver, it will use the schema
:   or instantiate and pass to dial method
:   b := grpc.RoundRobin(r)
:   conn, gerr := grpc.Dial("my-service", grpc.WithBalancer(b))


* Load balancing

Load balancing is done per call. Each call can end to different server.

How do we balance the load?

- Proxy
- Client side
- Client side with lookaside

Built in balancing strategies (pick first, round_robin, grpclb)

: So you have a list of url, and you have to decide which one to invoke.
: The client opens a subchannel for each url
: Problem: grpc is based on http/2. The pillar is maintaining a persistent connection.

* External Load Balancing

.image images/load-balancing.png 400 _

: You may not want to track the load of the servers, if they are alive and such.
: Separate responsabilities. Have an external service that do that.
: If the name resolution returns at least one load balancer address, the client switches to grpclb, delegating the balancing to an external service.

* External Load Balancing

The balancer is a Grpc endpoint itself:

    service LoadBalancer {
        // Bidirectional rpc to get a list of servers.
        rpc BalanceLoad(stream LoadBalanceRequest) returns (stream LoadBalanceResponse);
    }

: Note that it is a stream. The list may vary and get updated over time and also the requests
: The servers may want to communicate.
: The list is the order in which the calls must hit the servers.

* Other goodies

* Grpc gateway

[[https://github.com/grpc-ecosystem/grpc-gateway][Grpc gateway]]

    service YourService {
    rpc Echo(StringMessage) returns (StringMessage) {
        option (google.api.http) = {
        post: "/v1/example/echo"
        body: "*"
        };
    }

.image images/gprc-gateway.png 300 _

: annotate the service
: in case you need to support rest, or if you want to use some of the fancy tools for testing like pact
: haven't used it personally.

* Reflection (& tooling)

- Allows clients to build requests without having proto file compiled

    s := grpc.NewServer()
    pb.RegisterGreeterServer(s, &server{})
    reflection.Register(s)

Tools:

- [[https://github.com/fullstorydev/grpcurl][grpcurl]]
- [[https://github.com/fullstorydev/grpcui][grpcui]]

: one of the complains about rest vs grpc is that is easy to interact with a rest service via 
: curl
: the first is the equivalent of curl
: the second is a full fledget ui
: both allow to pass a proto file if we don't want to enable reflection on the server

* Conclusion

* Splitting the monolith

.image images/breaking.jpg 500 _

: Now I want to briefly discuss the most common use case for grpc
: The whole microservices movement starts with splitting a monolith.
: Even if you don't have a legacy one, the common practice is to start with a monolith to identify 
: domain boundaries which may eventually define different services
: Let's say we have a monolith we want to split
: Identify the seams (quote to sam newman / micheal feathers)
: With grpc is easy to make the boundaries interact. That's the easy part.
: The hard part is to handle the complexity
: Handle the fact that we need authentication, that we want to monitor, that the network is not reliable
: By using grpc, we leverage the experience of the giants in handling this kind of problems
