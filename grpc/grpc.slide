RPC on steroids with Go and Grpc

Federico Paolinelli
Red Hat
@fedepaol
fedepaol@gmail.com
fpaoline@redhat.com

* About me

- Red Hatter
- Doing distributed systems for more than 15 years
- CNV Networking team -> KubeVirt
- Open source lover

* Let's talk about RPC

* All but a new thing

- Corba
- Wsdl
- Java RMI
- Com (?)

:  allow function callers and function implementers to live in different processes
: the client and the server in an RPC exchange must be up and running at the same time.
: language indpendent definition of the Rpc -> IDL Interface domain language

* The need for communication

.image images/mailboxes.jpg 400 _
: TODO Creative commons
: demand for interoperability. Mobile also introduced need for efficiency
: not only Frontend / backend
: due to the complexity of our systems we need the appropriate tools. Go beyond simple remote invokation

* What options do I have?

- rest
- other rpcs (thrift, avro)
- websockets
- framed tcp
: rest is the king here. Not type safe. I know there are tools to generate the client /server but still
: websockets is not really a standard
: framed tcp: NIH syndrome

* Enters Grpc
: stands for Grpc remote procedure calls
: iteration of google's stubby, then opensourced

* Grpc & http2

Http2 allows multiplexing of requests over a single tcp connection (streams)

Grpc introduces channels, rpcs and messages:

.image images/grpc_on_http2_mapping_2.png 400 _

: Many streams on a single connection. On each stream, many messages (binary format)
: Advantage of a stream -> connection concurrency. Interleave messages on a single connection
: in go channel renamed to client conn
: channels enable multiple streams over multiple http2 connections (if load balance api is used)

* Serialization

* Protocol Buffers

*Types*definition*

    message Beer {
        int32 bid = 1;
        string beer_name = 2;
        string beer_description = 3;
        string beer_style = 4;
    }

*Services*definition*

    service BeersService {
        rpc GetBeer(BeerID) returns (Beer);
        rpc QueryBeer(BeerQueryParams) returns (stream Beer);
    }


: Default serialization mechanism for Grpc
: Run the protoc compiler and get data classes + serialization / deserialization
: sync call, streamed result, streamed in
: code generation for a lot of languages, interoperability

* Protobufs favour backward compatibility

- New fields can be added without breaking retro compatibility
- Intermediate service can parse the data, check what they need and propagate the data again

: be careful in setting the default values 

* Server side

    type BeersServiceServer interface {
        GetBeer(context.Context, *BeerID) (*Beer, error)
        QueryBeer(*BeerQueryParams, BeersService_QueryBeerServer) error
    }

*Client*side*
    
    // Generated code
    func NewBeersServiceClient(cc *grpc.ClientConn) BeersServiceClient
    func (c *beersServiceClient) GetBeer(ctx context.Context, in *BeerID, opts ...grpc.CallOption) (*Beer, error)

* Server implementation

    func (b *BeerServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        beerID := id.GetBid()
        beer, ok := b.beers[int(beerID)]
        if !ok {
            // from google.golang.org/grpc/status
            return nil, status.Error(codes.NotFound, "beer not found")
        }
        return &beer, nil
    }

: convenience method to avoid npes
: grpc comes with a grpc/status package to provide rich errors

* Server implementation (streaming)

    func (b *BeerServer) QueryBeer(p *beer.BeerQueryParams, 
                                   s beer.BeersService_QueryBeerServer) error {
        for _, b := range b.beers {
            if strings.Contains(b.BeerDescription, p.GetQuery()) {
                s.Send(&b)
            }
        }
        return nil
    }

* Establishing a connection

Client side:

    conn, _ := grpc.Dial(serverAddr, grpc.WithInsecure())
    client := beer.NewBeersServiceClient(conn)
       
Server side:

    lis, _ := net.Listen("tcp", port)
    server := grpc.NewServer()
    beer.RegisterBeersServiceServer(server, &BeerServer{})
    server.Serve(lis)

: Note that a connection can be in idle state, meaning that no rpc is invoked (or idle timeout has passed)
: a client connection can be used by many goroutines, all the gprc happen on separate goroutines
: there is also tls support 

* Keep alive

Not enabled by default, must use [[google.golang.org/grpc/keepalive]]

- Useful to detect disconnections where the endpoint hangs or dies
- Relies on http2 ping frames

Nice side effects:

- Keeps a pool of connections healthy
- Avoids killing idle of connections

: Tbh a really forgiving server side keep alive is enabled by default
: Needs to be enabled
: Some proxies kill connections where no traffic is passing

* Grpc Context gets propagated

* Deadline propagation

    ctx, cancel := context.WithTimeout(context.Background(), time.Second)
    beer, err := client.GetBeer(ctx, id)

.image images/propagation.png 200 _
    
    func (b *MiddleServer) GetBeer(ctx context.Context, id *beer.BeerID) (*beer.Beer, error) {
        if ctx.Err() == context.Canceled {
            return status.New(codes.Canceled, "Client cancelled, abandoning.")
        }
        b.client.GetBeer(ctx, id)
    }


: deadline sets a point in time, the same for all the calls
: with timeout sets a deadline in the future.
: if the same context gets propagated the calls will expire without wasting resources.
: the server can check if the client is still interested before wasting resources. Can decide to proceed, if the request is cached 
: and idempotency is implemented

* Cancellation

    ctx, cancel := context.WithCancel(context.Background())
    done := make(chan bool, 0)
    go func() {
        client.GetBeer(ctx, id)
        close(done)
    }()
    cancel()

.image images/cancellation.png 200 _


: cancellation gets propagated through te calls

* Metadata

Regular context values are not propagated.

[[google.golang.org/grpc/metadata]] is.

Client:

    metaData := metadata.Pairs("key", "value")
    ctx := metadata.NewContext(context.Background(), metaData)

Server:

    meta, ok := metadata.FromContext(ctx)
    value : = meta["key"][0]

: If it's business logic the api should be changed.
: metadata should carry only "infrastructure" payload
: no static checks, no code generated. To handle with care.

* Interceptors

Same as http middleware.

Two types:

- Unary Interceptors
- Stream Interceptors

Both Client side / server side

: I won't add the signature here since it's quite long
: can apply to server side and client side. Http generally intercept server side
: client unary. Pre processing, call, post processing
: client streaming. Pre processing, retrieve the stream, optionally decorate it with interceptior methods, return decorated stream
: server side is almost the same but instead of invoker you have a handler to pass the data / the stream

* Interceptors

To apply:

Client:

    conn, err := grpc.Dial(*addr, grpc.WithUnaryInterceptor(unaryInterceptor))

Server:

    s := grpc.NewServer(grpc.UnaryInterceptor(unaryInterceptor))

If we want more interceptors (client side):

    func WithChainUnaryInterceptor(interceptors ...UnaryClientInterceptor) DialOption

Server side not natively supported. Must use [[github.com/grpc-ecosystem/go-grpc-middleware]].

: they are per conn / per server. this means that all the calls on that conn will attemtp a retry, or 
: authenticate. If we want special cases, we need to handle them in our code. 

* Retry unary interceptor

    func RetryInterceptor(ctx context.Context, 
                          method string, 
                          req, reply interface{}, 
                          cc *grpc.ClientConn, 
                          invoker grpc.UnaryInvoker, 
                          opts ...grpc.CallOption) error {
        var lastErr error
        for attempt := uint(0); attempt < 10; attempt++ {
            lastErr = invoker(ctx, method, req, reply, cc, opts...)
            if lastErr == nil {
                return nil
            }
        }
        return lastErr
    }

: three phases. The actual call happens in invoker
: more real scenario. Only some errors are worth being retried (the other end needs to be idempotent. Think about about
: a payement service that is not idempotent.) Also, not all the calls can be idempotent. The caller needs to tell the info
: to the interceptor. Logic on method name, arguments passed in the context (the real context) or as metadata.


* Rate limiter interceptor

    func UnaryServerInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {
        if Limit() {
            return nil, status.Errorf(codes.ResourceExhausted, "%s is rejected by grpc_ratelimit middleare, please retry later.", info.FullMethod)
        }
        return handler(ctx, req)
    }


* Authentication

* Tracing

* Monitoring


* Json interoperability (grpc gateway?)